# -*- coding: utf-8 -*-
"""seflplay-rl-chess.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZXs8NoAaKIEeVbYkzefcpRrV-83cSaEP
"""

!pip install torch numpy python-chess tqdm

# Install Stockfish
!apt-get update
!apt-get install stockfish

# @title 2. Core Chess Environment and Utilities (Modified for AlphaZeroMoveMapper)

import chess
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from collections import Counter
import math
import random
import copy
from tqdm.notebook import tqdm


# --- AlphaZero-Style Move Mapper ---
ALPHAZERO_ACTION_SIZE = 64 * 73 # Fixed action space as per AlphaZero paper

class AlphaZeroMoveMapper:
    """
    Implements AlphaZero-style action encoding/decoding (64 * 73 action space).
    An action is represented by (from_square, move_type_idx_from_73_planes).

    The 73 move types are:
    0-55: 8 directions * 7 distances (for sliding pieces like Queen, Rook, Bishop)
    56-63: 8 Knight moves
    64-72: 9 Pawn underpromotion moves (3 types x 3 directions/targets: straight, diag-left, diag-right)
           (Queen promotions are handled by the sliding move type where pawn lands on promotion rank)
    """
    def __init__(self):
        # We pre-calculate the relative (dr, dc) and promotion type for each of the 73 abstract move types.
        self._type_idx_to_move_data = {} # Maps 0-72 type index to {'dr', 'dc', 'prom'}
        self._move_data_to_type_idx = {} # Reverse map for encoding: (dr, dc, prom) -> type_idx

        self._initialize_move_types()
        self.ACTION_SIZE = ALPHAZERO_ACTION_SIZE

    def _initialize_move_types(self):
        current_type_idx = 0

        # 1. Sliding Piece Moves (56 types)
        # 8 directions * 7 distances (1 to 7 squares away)
        directions = [
            (0, 1), (0, -1), (1, 0), (-1, 0),    # East, West, South, North (relative to current player's perspective)
            (1, 1), (1, -1), (-1, 1), (-1, -1)   # SE, SW, NE, NW (relative to current player's perspective)
        ]
        for dr, dc in directions:
            for dist in range(1, 8):
                self._type_idx_to_move_data[current_type_idx] = {'dr': dr * dist, 'dc': dc * dist, 'prom': None}
                self._move_data_to_type_idx[(dr * dist, dc * dist, None)] = current_type_idx
                current_type_idx += 1
        assert current_type_idx == 56, f"Sliding move types: {current_type_idx}"

        # 2. Knight Moves (8 types)
        knight_relative_moves = [
            (2, 1), (2, -1), (-2, 1), (-2, -1),
            (1, 2), (1, -2), (-1, 2), (-1, -2)
        ]
        for dr, dc in knight_relative_moves:
            self._type_idx_to_move_data[current_type_idx] = {'dr': dr, 'dc': dc, 'prom': None}
            self._move_data_to_type_idx[(dr, dc, None)] = current_type_idx
            current_type_idx += 1
        assert current_type_idx == 56 + 8, f"Sliding + Knight move types: {current_type_idx}" # 64

        # 3. Pawn Underpromotion Moves (9 types)
        # These are relative to the pawn's square, considering its movement (straight, diagonal left, diagonal right)
        # and the promotion piece (Rook, Bishop, Knight). Queen promotion is implicitly handled by sliding moves.
        pawn_promotion_patterns = [
            (1, 0),    # Straight push (to promotion rank)
            (1, -1),   # Diagonal left capture (to promotion rank)
            (1, 1)     # Diagonal right capture (to promotion rank)
        ]
        promotion_pieces = [chess.KNIGHT, chess.BISHOP, chess.ROOK] # Order matters for consistent indexing

        for dr_pattern, dc_pattern in pawn_promotion_patterns:
            for prom_piece in promotion_pieces:
                self._type_idx_to_move_data[current_type_idx] = {'dr': dr_pattern, 'dc': dc_pattern, 'prom': prom_piece}
                self._move_data_to_type_idx[(dr_pattern, dc_pattern, prom_piece)] = current_type_idx
                current_type_idx += 1
        assert current_type_idx == 64 + 9, f"Total move types: {current_type_idx}" # 73

    def move_to_action(self, move: chess.Move, board: chess.Board) -> int:
        """
        Encodes a chess.Move object to an AlphaZero-style integer action index.
        Action index = from_square_idx * 73 + move_type_idx.
        Requires the current board to determine relative move direction based on player perspective
        and to correctly identify pawn promotions.
        """
        from_sq = move.from_square
        to_sq = move.to_square
        prom_type = move.promotion # Will be None for non-promotions

        # Calculate relative row and column movement (dx, dy)
        dr = chess.square_rank(to_sq) - chess.square_rank(from_sq)
        dc = chess.square_file(to_sq) - chess.square_file(from_sq)

        # AlphaZero's internal representation flips the board for Black.
        # So, the relative move (dr, dc) must be converted to the current player's perspective.
        if board.turn == chess.BLACK:
            dr = -dr
            dc = -dc

        # Get the piece that is actually moving (needed for promotion logic)
        moving_piece_type = board.piece_at(from_sq).piece_type if board.piece_at(from_sq) else None

        move_type_idx = -1

        # Check for underpromotion moves first (highest specificity)
        if prom_type is not None and prom_type != chess.QUEEN and moving_piece_type == chess.PAWN:
            # For underpromotions, we look up the specific (dr, dc, prom_type)
            move_type_idx = self._move_data_to_type_idx.get((dr, dc, prom_type), -1)
            if move_type_idx != -1:
                return from_sq * 73 + move_type_idx

        # Check for regular moves (sliding, knight, pawn pushes/captures, queen promotions)
        # These are mapped by (dr, dc, None)
        # Queen promotions (pawn promotes to queen) are treated as regular sliding pawn moves.
        if (dr, dc, None) in self._move_data_to_type_idx:
            move_type_idx = self._move_data_to_type_idx[(dr, dc, None)]
            return from_sq * 73 + move_type_idx

        # --- Special Handling for Castling and En Passant ---
        # Castling: These are king moves that have specific (dr,dc).
        # e.g., White kingside castling (e1g1) is dr=0, dc=2.
        # e.g., White queenside castling (e1c1) is dr=0, dc=-2.
        # These should already be covered by the sliding moves (dist=2 along row/col).
        # En Passant: These are pawn moves (diagonal capture, dr=1, dc=+-1) with specific board conditions.
        # They should also be covered by the sliding moves (dist=1 along diagonal).

        # If we reach here, the move could not be mapped. This implies an issue in our definition
        # of the 73 types or in `chess.Move` interpretation.
        print(f"Warning: AlphaZeroMoveMapper: Move {move.uci()} (from_sq={from_sq}, to_sq={to_sq}, prom={prom_type}) with relative (dr={dr}, dc={dc}) could not be mapped to a type_idx.")
        return -1 # Indicate unmappable move

    def action_to_move(self, action_idx: int, board: chess.Board) -> chess.Move:
        """
        Decodes an AlphaZero-style integer action index back to a chess.Move object.
        Requires the current board to determine correct piece type for promotions and perspective.
        """
        from_sq = action_idx // 73
        type_idx = action_idx % 73

        if not (0 <= from_sq < 64 and 0 <= type_idx < 73):
            # print(f"Warning: AlphaZeroMoveMapper: Invalid action_idx {action_idx} (from_sq={from_sq}, type_idx={type_idx}) out of bounds.")
            return None

        type_data = self._type_idx_to_move_data.get(type_idx)
        if type_data is None:
            # print(f"Warning: AlphaZeroMoveMapper: Type index {type_idx} not found in _type_idx_to_move_data.")
            return None

        dr_relative_az = type_data['dr'] # dr from AZ perspective (current player at bottom)
        dc_relative_az = type_data['dc'] # dc from AZ perspective
        prom_type_az = type_data['prom'] # Promotion type if it's an underpromotion type

        # Reverse the perspective flip if it's Black's turn on the actual board
        dr_actual = dr_relative_az
        dc_actual = dc_relative_az
        if board.turn == chess.BLACK:
            dr_actual = -dr_actual
            dc_actual = -dc_actual

        to_rank = chess.square_rank(from_sq) + dr_actual
        to_file = chess.square_file(from_sq) + dc_actual

        if not (0 <= to_rank <= 7 and 0 <= to_file <= 7):
            return None # Target square out of bounds for an 8x8 board

        to_sq = chess.square(to_file, to_rank)

        # Handle promotions
        # Determine if the move should be a promotion.
        # This is based on the piece at from_sq and the target rank (to_sq).
        piece_at_from_sq = board.piece_at(from_sq)
        is_pawn_moving = piece_at_from_sq and piece_at_from_sq.piece_type == chess.PAWN
        is_promotion_rank = (piece_at_from_sq and piece_at_from_sq.color == chess.WHITE and chess.square_rank(to_sq) == 7) or \
                            (piece_at_from_sq and piece_at_from_sq.color == chess.BLACK and chess.square_rank(to_sq) == 0)


        if is_pawn_moving and is_promotion_rank:
            # If the action type specifies an underpromotion (prom_type_az is not None)
            if prom_type_az:
                return chess.Move(from_sq, to_sq, promotion=prom_type_az)
            else:
                # If it's a pawn move to the promotion rank but the action type doesn't specify underpromotion,
                # then it's a Queen promotion (as Queen promotions are not explicitly in the 9 types).
                return chess.Move(from_sq, to_sq, promotion=chess.QUEEN)

        # For all other moves (non-pawn or non-promotion pawn moves)
        return chess.Move(from_sq, to_sq)

# Instantiate the new mapper
az_move_mapper = AlphaZeroMoveMapper()

# Update the global ACTION_SIZE and the mapper instance used everywhere
ACTION_SIZE = az_move_mapper.ACTION_SIZE
move_mapper = az_move_mapper # This line replaces the old move_mapper = MoveMapper()
print(f"Updated global ACTION_SIZE to: {ACTION_SIZE}")


# @title 2. Core Chess Environment and Utilities (Modified for AlphaZeroMoveMapper)
# ... (rest of the imports and AlphaZeroMoveMapper class) ...


# @title ChessEnv Definition (This is the definition you asked for)

class ChessEnv:
    """
    Simplified Chess Environment using python-chess.
    Provides observations and handles game state for self-play.
    """
    def __init__(self):
        self.board = chess.Board()
        self.current_player = chess.WHITE

    def reset(self):
        self.board = chess.Board()
        self.current_player = chess.WHITE
        return self._get_observation()

    def _get_observation(self):
        obs = np.zeros((8, 8, 20), dtype=np.float32)
        piece_type_to_channel = {
            chess.PAWN: 0, chess.KNIGHT: 1, chess.BISHOP: 2,
            chess.ROOK: 3, chess.QUEEN: 4, chess.KING: 5
        }
        current_player_channels_base = 0
        opponent_channels_base = 6
        for sq in chess.SQUARES:
            piece = self.board.piece_at(sq)
            if piece:
                row, col = chess.square_rank(sq), chess.square_file(sq)
                if self.board.turn == chess.BLACK:
                    row = 7 - row
                    col = 7 - col
                if piece.color == self.board.turn:
                    obs[row, col, current_player_channels_base + piece_type_to_channel[piece.piece_type]] = 1.0
                else:
                    obs[row, col, opponent_channels_base + piece_type_to_channel[piece.piece_type]] = 1.0
        obs[:, :, 12] = 1.0 if self.board.turn == chess.WHITE else 0.0
        return obs

    def step(self, action_idx: int):
        """
        Applies an action (given by its numerical index) to the board.
        Returns the new observation, a reward (if game ends), a 'done' flag, and info.
        """
        # FIX: Pass self.board to action_to_move here
        move = move_mapper.action_to_move(action_idx, self.board)

        if move is None or move not in self.board.legal_moves:
            print(f"Warning: Illegal move {move.uci() if move else 'None'} from action index {action_idx} attempted for FEN: {self.board.fen()}")
            reward = -1.0
            done = True
            info = {"illegal_move": True}
            return self._get_observation(), reward, done, info

        self.board.push(move)
        self.current_player = self.board.turn

        reward = self._get_reward()
        done = self.board.is_game_over(claim_draw=True)

        info = {}
        if done:
            outcome = self.board.outcome(claim_draw=True)
            if outcome:
                info["winner"] = outcome.winner
                info["termination"] = outcome.termination

        return self._get_observation(), reward, done, info

    def _get_reward(self):
        outcome = self.board.outcome(claim_draw=True)
        if outcome is None:
            return 0.0
        if outcome.winner == chess.WHITE:
            return 1.0
        elif outcome.winner == chess.BLACK:
            return -1.0
        else:
            return 0.0

    def is_game_over(self):
        return self.board.is_game_over(claim_draw=True)

    def get_legal_actions_mask(self):
        mask = torch.zeros(ACTION_SIZE, dtype=torch.bool)
        for move in self.board.legal_moves:
            # This already passes board, it was correct
            action_idx = move_mapper.move_to_action(move, self.board)
            if action_idx != -1:
                mask[action_idx] = True
        return mask

    def get_current_player(self):
        return self.board.turn

# @title 3. Neural Network (ChessNet) - No changes needed here, as ACTION_SIZE is global

# Using the ACTION_SIZE from the updated AlphaZeroMoveMapper
class ResBlock(nn.Module):
    def __init__(self, c):
        super().__init__()
        self.conv1 = nn.Conv2d(c,c,3,padding=1); self.bn1 = nn.BatchNorm2d(c)
        self.conv2 = nn.Conv2d(c,c,3,padding=1); self.bn2 = nn.BatchNorm2d(c)
    def forward(self, x):
        out = F.relu(self.bn1(self.conv1(x)))
        out = self.bn2(self.conv2(out))
        return F.relu(x + out)

class ChessNet(nn.Module):
    def __init__(self, blocks=5, c=128):
        super().__init__()
        # Input channels must match our ChessEnv._get_observation (which is 20)
        self.conv = nn.Conv2d(20, c, 3, padding=1); self.bn = nn.BatchNorm2d(c)
        self.blocks = nn.Sequential(*[ResBlock(c) for _ in range(blocks)])
        self.policy_conv = nn.Conv2d(c,2,1); self.policy_bn = nn.BatchNorm2d(2)
        self.policy_fc = nn.Linear(2*8*8, ACTION_SIZE) # ACTION_SIZE from global
        self.value_conv = nn.Conv2d(c,1,1); self.value_bn = nn.BatchNorm2d(1)
        self.value_fc1 = nn.Linear(1*8*8, c); self.value_fc2 = nn.Linear(c,1)

    def forward(self, x):
        x = F.relu(self.bn(self.conv(x)))
        x = self.blocks(x)
        p = F.relu(self.policy_bn(self.policy_conv(x)))
        p = self.policy_fc(p.reshape(x.size(0), -1))
        p = F.log_softmax(p, dim=1) # Log softmax for policy head (for NLLLoss)
        v = F.relu(self.value_bn(self.value_conv(x)))
        v = F.relu(self.value_fc1(v.reshape(x.size(0), -1)))
        v = torch.tanh(self.value_fc2(v)) # Tanh for value head (-1 to 1)
        return p, v

# @title 4. Monte Carlo Tree Search (MCTS) - Modified for AlphaZeroMoveMapper

class MCTS:
    def __init__(self, net, cpuct=1.5, sims=50):
        self.net = net
        self.cpuct = cpuct
        self.sims = sims
        self.Q = {}
        self.N = {}
        self.P = {}
        self.Ns = {}

    def _get_obs_tensor(self, board: chess.Board, current_player: bool, device):
        temp_env = ChessEnv()
        temp_env.board = board
        temp_env.current_player = current_player
        obs_np = temp_env._get_observation()
        return torch.tensor(obs_np).permute(2, 0, 1).float().unsqueeze(0).to(device)

    def _get_legal_actions_mask(self, board: chess.Board):
        mask = torch.zeros(ACTION_SIZE, dtype=torch.bool)
        for move in board.legal_moves:
            # Pass board to move_to_action
            action_idx = move_mapper.move_to_action(move, board)
            if action_idx != -1:
                mask[action_idx] = True
        return mask

    def _expand_node(self, fen: str, board: chess.Board, current_player: bool, device):
        obs_tensor = self._get_obs_tensor(board, current_player, device)
        self.net.eval()
        with torch.no_grad():
            log_probs, value = self.net(obs_tensor)

        legal_actions_mask = self._get_legal_actions_mask(board)
        legal_action_indices = torch.where(legal_actions_mask)[0].cpu().numpy()

        self.P[fen] = {}
        for action_idx in legal_action_indices:
            if action_idx < log_probs.size(1):
                self.P[fen][action_idx] = log_probs[0, action_idx].exp().item()
            # else: (Debug warning for unmapped action indices)

        if not self.P[fen] and not board.is_game_over(claim_draw=True):
             # print(f"Warning: No legal actions found for FEN {fen}, but board not game over. Initializing P[fen] as empty.")
             self.P[fen] = {}

        self.Ns[fen] = 0
        for action in self.P[fen].keys():
            self.Q[(fen, action)] = 0.0
            self.N[(fen, action)] = 0

        return value.item()

    def simulate(self, board: chess.Board):
        path = []
        current_board = board.copy()
        device = next(self.net.parameters()).device
        initial_simulation_player = current_board.turn

        while True:
            fen = current_board.fen()
            is_game_over = current_board.is_game_over(claim_draw=True)

            if is_game_over:
                outcome = current_board.outcome(claim_draw=True)
                if outcome.winner == initial_simulation_player: value = 1.0
                elif outcome.winner is None: value = 0.0
                else: value = -1.0
                break

            if fen not in self.P:
                value = self._expand_node(fen, current_board, current_board.turn, device)
                if current_board.turn != initial_simulation_player: value = -value
                break

            best_action = -1
            best_ucb_value = -float('inf')

            legal_actions_mask = self._get_legal_actions_mask(current_board)
            legal_action_indices = torch.where(legal_actions_mask)[0].cpu().numpy()

            if not legal_action_indices.size > 0:
                value = 0.0
                break

            current_node_total_visits = self.Ns.get(fen, 0)

            for action_idx in legal_action_indices:
                q_sa = self.Q.get((fen, action_idx), 0.0)
                n_sa = self.N.get((fen, action_idx), 0)
                p_sa = self.P[fen].get(action_idx, 0.0)

                ucb_value = q_sa + self.cpuct * p_sa * math.sqrt(current_node_total_visits) / (1 + n_sa)

                if ucb_value > best_ucb_value:
                    best_ucb_value = ucb_value
                    best_action = action_idx

            if best_action == -1:
                value = 0.0
                break

            path.append((fen, best_action, current_board.turn))

            # Pass board to action_to_move
            move = move_mapper.action_to_move(best_action, current_board)
            if move is None: # Critical check for unmappable actions
                print(f"MCTS Simulate Error: Failed to decode action {best_action} for FEN {current_board.fen()}. Terminating simulation path.")
                value = 0.0 # Treat as error/draw for this path
                break

            try:
                current_board.push(move)
            except ValueError as e:
                print(f"MCTS Simulate Error: Illegal move {move.uci()} push detected (action {best_action}). FEN: {current_board.fen()}. Error: {e}. Terminating simulation path.")
                value = 0.0 # Treat as error/draw for this path
                break

        for fen_node, action_node, player_at_node_turn in reversed(path):
            self.Ns[fen_node] = self.Ns.get(fen_node, 0) + 1
            self.N[(fen_node, action_node)] = self.N.get((fen_node, action_node), 0) + 1

            reward_for_this_node_player = value if player_at_node_turn == initial_simulation_player else -value
            self.Q[(fen_node, action_node)] = (self.N[(fen_node, action_node)] * self.Q.get((fen_node, action_node), 0.0) + reward_for_this_node_player) / self.N[(fen_node, action_node)]
            value = -value

    def get_pi(self, env: ChessEnv, temp=1.0):
        initial_board = env.board.copy()
        initial_fen = initial_board.fen()

        self.Q = {}
        self.N = {}
        self.P = {}
        self.Ns = {}

        if initial_fen not in self.P:
            self._expand_node(initial_fen, initial_board, initial_board.turn, next(self.net.parameters()).device)

        legal_actions_mask = self._get_legal_actions_mask(initial_board)
        legal_action_indices = torch.where(legal_actions_mask)[0].cpu().numpy()

        if legal_action_indices.size > 0:
            dirichlet_noise = np.random.dirichlet([0.3] * len(legal_action_indices))
            for i, action_idx in enumerate(legal_action_indices):
                if action_idx in self.P[initial_fen]:
                    self.P[initial_fen][action_idx] = (0.75 * self.P[initial_fen][action_idx] +
                                                       0.25 * dirichlet_noise[i])
            sum_p = sum(self.P[initial_fen].values())
            if sum_p > 0:
                for action_idx in self.P[initial_fen]:
                    self.P[initial_fen][action_idx] /= sum_p

        for _ in range(self.sims):
            self.simulate(initial_board.copy())

        counts = [self.N.get((initial_fen, a), 0) for a in legal_action_indices]

        pi = {}
        if temp == 0:
            if not legal_action_indices.size > 0: return {}
            if all(c == 0 for c in counts):
                num_actions = len(legal_action_indices)
                if num_actions > 0: pi = {a: 1.0/num_actions for a in legal_action_indices}
                else: pi = {}
            else:
                best_action_idx = legal_action_indices[np.argmax(counts)]
                for action in legal_action_indices:
                    pi[action] = 1.0 if action == best_action_idx else 0.0
        else:
            exps = [c**(1/temp) for c in counts]
            total_exp = sum(exps)
            if total_exp == 0:
                num_actions = len(legal_action_indices)
                if num_actions > 0: pi = {a: 1.0/num_actions for a in legal_action_indices}
                else: pi = {}
            else:
                pi = {a: exp/total_exp for a, exp in zip(legal_action_indices, exps)}
        return pi

# @title 5. Self-Play and Training Loop (with Debug Logs)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

net = ChessNet().to(device)
mcts = MCTS(net, sims=50) # Reduced simulations for faster training on Colab

optimizer = torch.optim.Adam(net.parameters(), lr=1e-4, weight_decay=1e-4)

EPOCHS = 100 # Number of full training cycles
GAMES_PER_EPOCH = 50 # Number of self-play games per epoch (increase for better results)

all_game_data = [] # Stores (observation_tensor_numpy, pi_dict, value) for training

print("\n--- Starting Self-Play & Training Cycles ---")
for epoch in range(EPOCHS):
    print(f"\n--- Epoch {epoch+1}/{EPOCHS} ---")
    game_losses = [] # To track actual game outcomes for the epoch

    for g in tqdm(range(GAMES_PER_EPOCH), desc=f"Self-Play Games (Epoch {epoch+1})"):
        print(f"\n--- Starting Self-Play Game {g+1}/{GAMES_PER_EPOCH} ---")
        env = ChessEnv()
        _ = env.reset() # Initial observation, not used immediately
        game_data = [] # (observation_tensor_numpy, pi_dict, player_at_state_turn) for this single game

        turn_counter = 0
        MAX_MOVES_PER_GAME = 150 # Prevent excessively long games

        while not env.is_game_over() and turn_counter < MAX_MOVES_PER_GAME:
            current_obs = env._get_observation() # Get current observation for the state
            current_player_turn = env.get_current_player() # Get current player (chess.WHITE or chess.BLACK)
            current_fen = env.board.fen()
            print(f"  Game {g+1} Turn {turn_counter+1} ({'White' if current_player_turn == chess.WHITE else 'Black'}): FEN: {current_fen}")

            # Get policy from MCTS (temperature > 0 for exploration)
            temp = 1.0 if turn_counter < 10 else 0.5 # Anneal temp for more greedy play later
            print(f"    Running MCTS with {mcts.sims} simulations, temp={temp}...")
            pi = mcts.get_pi(env, temp=temp) # MCTS instance will manage its own internal state

            if not pi: # No legal moves found by MCTS, potentially terminal state or bug
                if env.is_game_over():
                    print(f"    Game {g+1}: MCTS returned empty policy. Game detected as over. Breaking loop.")
                    break # Game is truly over
                else:
                    print(f"    Game {g+1}: MCTS returned empty policy unexpectedly, but game is not over. FEN: {env.board.fen()}. Breaking loop.")
                    break

            # Store data for training: (observation_numpy, policy_dict, player_turn_at_this_state)
            game_data.append([current_obs, pi, current_player_turn])

            # Choose action based on MCTS policy (probabilistic choice)
            actions = list(pi.keys())
            probabilities = list(pi.values())

            # Handle cases where probabilities might sum to zero or actions are empty
            if not actions or sum(probabilities) == 0:
                print(f"    Warning: No actions or zero probabilities from MCTS for game {g+1}. Skipping move. FEN: {env.board.fen()}. Breaking loop.")
                break

            # Ensure probabilities sum to 1 before sampling
            sum_probs = sum(probabilities)
            if sum_probs > 0:
                probabilities = [p / sum_probs for p in probabilities]
            else:
                num_actions = len(actions)
                if num_actions > 0:
                    probabilities = [1.0 / num_actions] * num_actions
                else:
                    print(f"    Error: No actions to sample from for game {g+1}. Breaking loop.")
                    break

            chosen_action = random.choices(actions, weights=probabilities, k=1)[0]
            chosen_move = move_mapper.action_to_move(chosen_action, env.board)
            print(f"    Selected Action: {chosen_action} (Move: {chosen_move.uci() if chosen_move else 'Invalid'})")

            # Step the environment with the chosen action
            _, _, _, info = env.step(chosen_action) # _ means we don't use immediate reward/done
            turn_counter += 1

            if "illegal_move" in info and info["illegal_move"]:
                print(f"    Game {g+1}: Encountered illegal move. Game terminated early by environment. FEN: {env.board.fen()}")
                break

        # --- Game is finished, calculate final result and assign rewards ---
        final_board_outcome = env.board.outcome(claim_draw=True)
        game_outcome_str = env.board.result(claim_draw=True) # e.g., "1-0", "0-1", "1/2-1/2"
        print(f"  Game {g+1} Finished. Outcome: {game_outcome_str}. Final FEN: {env.board.fen()}")
        game_losses.append(game_outcome_str) # Record the outcome for summary

        for obs_np, pi_dict, player_at_state_turn in game_data:
            outcome_for_state_player = 0.0 # Default for draw

            if final_board_outcome:
                if final_board_outcome.winner == player_at_state_turn:
                     outcome_for_state_player = 1.0 # The player at this state won the game
                elif final_board_outcome.winner is not None and final_board_outcome.winner != player_at_state_turn:
                     outcome_for_state_player = -1.0 # The player at this state lost the game
                # Else (winner is None), it's a draw, outcome_for_state_player remains 0.0
            else:
                # If final_board_outcome is None (game terminated early by max moves or illegal move without clear outcome)
                # Treat as a draw for training purposes or assign 0. This can be refined.
                outcome_for_state_player = 0.0

            all_game_data.append((obs_np, pi_dict, outcome_for_state_player))

    print(f"\n--- Epoch {epoch+1} Self-Play Summary ---")
    outcome_counts = Counter(game_losses)
    print(f"  Game Outcomes: {outcome_counts}")

    # --- Training Phase ---
    if not all_game_data:
        print(f"Epoch {epoch+1}: No game data collected for training. Skipping training phase.")
        continue

    print(f"\n--- Starting Training for Epoch {epoch+1} ---")
    print(f"  Training on {len(all_game_data)} collected states.")
    net.train() # Set network to training mode
    total_loss = 0.0

    observations_batch = np.array([item[0] for item in all_game_data]) # (N, H, W, C)
    observations_tensor = torch.tensor(observations_batch).float().permute(0, 3, 1, 2).to(device) # (N, C, H, W)

    policies_tensor = torch.zeros(len(all_game_data), ACTION_SIZE, dtype=torch.float32).to(device)
    for i, item in enumerate(all_game_data):
        pi_dict = item[1]
        for action_idx, prob in pi_dict.items():
            if action_idx < ACTION_SIZE:
                policies_tensor[i, action_idx] = prob
            # else: print(f"Warning: Policy action_idx {action_idx} out of bounds for ACTION_SIZE {ACTION_SIZE}")

    rewards_tensor = torch.tensor([item[2] for item in all_game_data], dtype=torch.float32).to(device)

    dataset = torch.utils.data.TensorDataset(observations_tensor, policies_tensor, rewards_tensor)
    data_loader = torch.utils.data.DataLoader(dataset, batch_size=64, shuffle=True)

    batch_count = 0
    for obs_batch, pi_batch, z_batch in tqdm(data_loader, desc=f"Training Batches (Epoch {epoch+1})"):
        batch_count += 1
        p_pred, v_pred = net(obs_batch) # p_pred is log_softmax, v_pred is tanh

        value_loss = F.mse_loss(v_pred.view(-1), z_batch)
        policy_loss = -torch.sum(pi_batch * p_pred, dim=1).mean()

        loss = value_loss + policy_loss

        optimizer.zero_grad()
        loss.backward() # This is where backpropagation happens!
        optimizer.step()

        total_loss += loss.item()
        if batch_count % 10 == 0: # Log every 10 batches
            print(f"    Batch {batch_count}/{len(data_loader)} - Loss: {loss.item():.4f} (Value: {value_loss.item():.4f}, Policy: {policy_loss.item():.4f})")


    avg_loss = total_loss / len(data_loader) if len(data_loader) > 0 else 0.0
    print(f"Epoch {epoch+1} Training Finished. Average Batch Loss = {avg_loss:.4f}")

    all_game_data.clear() # Clear data after training epoch for the next epoch's self-play

# --- Save the trained model ---
torch.save(net.state_dict(), 'chess_alphazero_mini.pth')
print("\n--- Training Complete ---")
print("Model saved to chess_alphazero_mini.pth")

# @title 6. Evaluation with Stockfish (Modified for AlphaZeroMoveMapper)

import chess.engine

# Load the trained model weights
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
loaded_net = ChessNet().to(device)
loaded_net.load_state_dict(torch.load('chess_alphazero_mini.pth', map_location=device))
loaded_net.eval()
print("Model weights loaded successfully.")

eval_mcts = MCTS(loaded_net, sims=50)

def evaluate_with_stockfish(model_mcts, engine_path, stockfish_elo, games=10):
    results = Counter({"White Wins": 0, "Black Wins": 0, "Draws": 0, "Errors": 0})
    engine = None
    try:
        engine = chess.engine.SimpleEngine.popen_uci(engine_path)
        if stockfish_elo < 1350:
            stockfish_elo = 1350
            print(f"  Warning: Stockfish UCI_Elo minimum is 1350. Adjusting {stockfish_elo} to 1350.")
        engine.configure({"UCI_Elo": stockfish_elo})
        print(f"Stockfish engine started with ELO: {stockfish_elo}")
    except Exception as e:
        print(f"Error starting Stockfish engine: {e}. All games marked as errors.")
        results["Errors"] += games
        return results

    print(f"\n--- Starting {games} Evaluation Games vs Stockfish ELO {stockfish_elo} ---")
    for i in tqdm(range(games), desc=f"Evaluating vs Stockfish ELO {stockfish_elo}"):
        env = ChessEnv()
        _ = env.reset()
        board = env.board.copy()
        game_result_str = "In Progress"
        print(f"  Game {i+1}/{games} started. Initial FEN: {board.fen()}")

        MAX_MOVES_EVAL_GAME = 100

        for move_num in range(MAX_MOVES_EVAL_GAME):
            if board.is_game_over(claim_draw=True):
                print(f"    Game {i+1} concluded due to game over condition.")
                break

            current_player = board.turn
            selected_move = None
            player_str = "White (Model)" if current_player == chess.WHITE else "Black (Stockfish)"
            print(f"    Move {move_num+1}: {player_str}'s turn. Current FEN: {board.fen()}")

            if current_player == chess.WHITE:
                print(f"      Model MCTS: Running simulations for White's turn...")
                pi = model_mcts.get_pi(env, temp=0)

                if not pi:
                    if board.is_game_over(claim_draw=True):
                        print(f"      Model MCTS: Game over detected before move selection. Breaking.")
                        break
                    else:
                        print(f"      Model MCTS: Unexpectedly found no legal moves. This indicates an issue. Breaking.")
                        game_result_str = "Error: White no legal moves"
                        results["Errors"] += 1
                        break

                best_action_idx = max(pi, key=pi.get)
                # Pass board to action_to_move
                selected_move = move_mapper.action_to_move(best_action_idx, board)
                print(f"      Model MCTS selected: Action {best_action_idx} (Move: {selected_move.uci() if selected_move else 'Invalid'})")
                if selected_move is None: # Critical check
                    print(f"      Model MCTS Error: Decoded move is None. This implies an encoding/decoding issue. Breaking.")
                    game_result_str = "Error: Model decoding issue"
                    results["Errors"] += 1
                    break

            else:
                try:
                    print(f"      Stockfish: Requesting move (limit=0.05s)...")
                    engine_result = engine.play(board, limit=chess.engine.Limit(time=0.05))
                    selected_move = engine_result.move
                    print(f"      Stockfish selected: Move {selected_move.uci()}")
                except chess.engine.EngineError as e:
                    print(f"      Stockfish error during game {i+1}: {e}. Board FEN: {board.fen()}. Terminating game.")
                    game_result_str = "Error: Stockfish engine"
                    results["Errors"] += 1
                    break
                except Exception as e:
                    print(f"      Unexpected error from Stockfish: {e}. Board FEN: {board.fen()}. Terminating game.")
                    game_result_str = "Error: Stockfish execution"
                    results["Errors"] += 1
                    break

            if selected_move and selected_move in board.legal_moves:
                board.push(selected_move)
                env.board = board.copy()
                print(f"    Move {selected_move.uci()} applied successfully.")
            else:
                print(f"    ERROR: {'White' if current_player == chess.WHITE else 'Black'} attempted illegal/null move: {selected_move}. FEN: {board.fen()}. Terminating game.")
                game_result_str = f"Error: {'White' if current_player == chess.WHITE else 'Black'} illegal move"
                results["Errors"] += 1
                break

        if board.is_game_over(claim_draw=True):
            outcome = board.outcome(claim_draw=True)
            if outcome.winner == chess.WHITE:
                results["White Wins"] += 1
                game_result_str = "White Wins (Checkmate/Resignation)"
            elif outcome.winner == chess.BLACK:
                results["Black Wins"] += 1
                game_result_str = "Black Wins (Checkmate/Resignation)"
            else:
                results["Draws"] += 1
                game_result_str = "Draw (Stalemate/Repetition/50-move rule)"
        else:
            results["Draws"] += 1
            game_result_str = "Draw (Max Moves Reached / Early Break)"

        print(f"  Game {i+1}/{games} finished: {game_result_str}. Final FEN: {board.fen()}\n")

    if engine:
        engine.quit()
    return results

# Define ELO levels for evaluation
elo_levels = [1350, 1400, 1600, 1800, 2000]

benchmark_results = {}

print("\n--- Starting Model Evaluation ---")
for elo in elo_levels:
    results = evaluate_with_stockfish(eval_mcts, "/usr/games/stockfish", elo, games=2)
    benchmark_results[elo] = results
    print(f"Results for Stockfish ELO {elo}: {results}")

print("\n--- Overall Benchmark Results ---")
for elo, results in benchmark_results.items():
    print(f"ELO {elo}: {results}")

# @title 7. Hyperparameter Optimization & Analysis

import matplotlib.pyplot as plt
import pandas as pd
from collections import Counter
import gc # For garbage collection

# --- Configuration for Hyperparameter Search ---
# Define combinations of (num_res_blocks, num_channels) to test
HYPERPARAM_COMBINATIONS = [
    (1, 64),   # Very small network
    (3, 64),   # Small network, modest depth
    (5, 64),   # Your current baseline, reduced channels
    (3, 128),  # Modest depth, more channels
    (5, 128),  # Your current baseline
    (7, 128),  # Deeper network, more channels
    # (5, 256),  # More channels, might be too large for free tier
    # (10, 128), # Deeper network, might be too large for free tier
]

# Training parameters for each combination (reduced for hyperparam search speed)
SEARCH_EPOCHS = 1      # Number of training epochs per combination
SEARCH_GAMES_PER_EPOCH = 3 # Number of self-play games per epoch per combination
SEARCH_EVAL_GAMES = 2  # Number of evaluation games against Stockfish per combination

# Stockfish ELO for evaluation (choose a mid-range ELO)
EVAL_STOCKFISH_ELO = 1400 # Or 1350, as it's the minimum.

# --- Storage for Results ---
results_data = []

print("--- Starting Hyperparameter Search ---")

for blocks, channels in HYPERPARAM_COMBINATIONS:
    print(f"\n--- Testing Combination: Blocks={blocks}, Channels={channels} ---")

    # 1. Initialize fresh network, MCTS, and optimizer for this combination
    current_net = ChessNet(blocks=blocks, c=channels).to(device)
    current_mcts = MCTS(current_net, sims=50) # Keep sims consistent for comparison
    current_optimizer = torch.optim.Adam(current_net.parameters(), lr=1e-4, weight_decay=1e-4)

    # 2. Run simplified Self-Play and Training Loop
    all_game_data_combo = [] # Specific data for this combination's training
    total_training_loss_combo = 0.0

    for epoch in range(SEARCH_EPOCHS):
        print(f"  Training Epoch {epoch+1}/{SEARCH_EPOCHS} for Blocks={blocks}, Channels={channels}")
        for g in tqdm(range(SEARCH_GAMES_PER_EPOCH), desc=f"  Self-Play Games"):
            env = ChessEnv()
            _ = env.reset()
            game_data = []

            turn_counter = 0
            MAX_MOVES_PER_GAME = 150

            while not env.is_game_over() and turn_counter < MAX_MOVES_PER_GAME:
                current_obs = env._get_observation()
                current_player_turn = env.get_current_player()

                temp = 1.0 if turn_counter < 10 else 0.5
                pi = current_mcts.get_pi(env, temp=temp) # Use current_mcts instance

                if not pi:
                    if env.is_game_over(): break
                    else: break

                game_data.append([current_obs, pi, current_player_turn])
                actions = list(pi.keys())
                probabilities = list(pi.values())
                sum_probs = sum(probabilities)
                if sum_probs > 0: probabilities = [p / sum_probs for p in probabilities]
                else:
                    num_actions = len(actions)
                    if num_actions > 0: probabilities = [1.0 / num_actions] * num_actions
                    else: break

                chosen_action = random.choices(actions, weights=probabilities, k=1)[0]
                _, _, _, info = env.step(chosen_action)
                turn_counter += 1
                if "illegal_move" in info and info["illegal_move"]: break

            final_board_outcome = env.board.outcome(claim_draw=True)
            for obs_np, pi_dict, player_at_state_turn in game_data:
                outcome_for_state_player = 0.0
                if final_board_outcome:
                    if final_board_outcome.winner == player_at_state_turn: outcome_for_state_player = 1.0
                    elif final_board_outcome.winner is not None and final_board_outcome.winner != player_at_state_turn: outcome_for_state_player = -1.0
                all_game_data_combo.append((obs_np, pi_dict, outcome_for_state_player))

        # Training phase for this epoch and combo
        if all_game_data_combo:
            current_net.train()
            observations_batch = np.array([item[0] for item in all_game_data_combo])
            observations_tensor = torch.tensor(observations_batch).float().permute(0, 3, 1, 2).to(device)
            policies_tensor = torch.zeros(len(all_game_data_combo), ACTION_SIZE, dtype=torch.float32).to(device)
            for i, item in enumerate(all_game_data_combo):
                pi_dict = item[1]
                for action_idx, prob in pi_dict.items():
                    if action_idx < ACTION_SIZE: policies_tensor[i, action_idx] = prob
            rewards_tensor = torch.tensor([item[2] for item in all_game_data_combo], dtype=torch.float32).to(device)

            dataset = torch.utils.data.TensorDataset(observations_tensor, policies_tensor, rewards_tensor)
            data_loader = torch.utils.data.DataLoader(dataset, batch_size=64, shuffle=True)

            current_epoch_loss = 0.0
            for obs_batch, pi_batch, z_batch in data_loader:
                p_pred, v_pred = current_net(obs_batch)
                value_loss = F.mse_loss(v_pred.view(-1), z_batch)
                policy_loss = -torch.sum(pi_batch * p_pred, dim=1).mean()
                loss = value_loss + policy_loss
                current_optimizer.zero_grad()
                loss.backward()
                current_optimizer.step()
                current_epoch_loss += loss.item()
            total_training_loss_combo = current_epoch_loss / len(data_loader) if len(data_loader) > 0 else 0.0
        else:
            total_training_loss_combo = float('nan') # No data, no loss

    # 3. Evaluate the trained network for this combination
    print(f"  Evaluating Blocks={blocks}, Channels={channels}...")
    # Make sure to pass the current_mcts which uses current_net
    eval_results = evaluate_with_stockfish(current_mcts, "/usr/games/stockfish", EVAL_STOCKFISH_ELO, games=SEARCH_EVAL_GAMES)

    # Store all results
    results_data.append({
        'blocks': blocks,
        'channels': channels,
        'avg_training_loss': total_training_loss_combo,
        'white_wins': eval_results['White Wins'],
        'black_wins': eval_results['Black Wins'],
        'draws': eval_results['Draws'],
        'errors': eval_results['Errors']
    })

    # Clear memory explicitly
    del current_net, current_mcts, current_optimizer, all_game_data_combo
    if 'observations_tensor' in locals(): del observations_tensor, policies_tensor, rewards_tensor, dataset, data_loader
    torch.cuda.empty_cache() # Clear CUDA memory if on GPU
    gc.collect() # Force garbage collection

    print(f"  Results for Blocks={blocks}, Channels={channels}:")
    print(f"    Avg Training Loss: {total_training_loss_combo:.4f}")
    print(f"    Evaluation vs Stockfish {EVAL_STOCKFISH_ELO}: White Wins={eval_results['White Wins']}, Black Wins={eval_results['Black Wins']}, Draws={eval_results['Draws']}, Errors={eval_results['Errors']}")

# --- Analysis and Plotting ---
print("\n--- Hyperparameter Search Complete ---")
df_results = pd.DataFrame(results_data)
print("\nSummary of Results:")
print(df_results)

# --- Plotting Results ---

# Create a combined identifier for plotting
df_results['Combination'] = df_results.apply(lambda row: f"B{row['blocks']} C{row['channels']}", axis=1)

# Sort for better visualization
df_results = df_results.sort_values(by=['blocks', 'channels'])

# Plotting Training Loss
plt.figure(figsize=(10, 6))
plt.bar(df_results['Combination'], df_results['avg_training_loss'], color='skyblue')
plt.xlabel('Network Combination (Blocks, Channels)')
plt.ylabel('Average Training Loss')
plt.title('Average Training Loss for Different Network Combinations')
plt.xticks(rotation=45, ha='right')
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.tight_layout()
plt.show()

# Plotting Evaluation Results (Wins/Draws)
fig, ax = plt.subplots(figsize=(12, 7))

bar_width = 0.25
index = np.arange(len(df_results['Combination']))

bar1 = ax.bar(index - bar_width, df_results['white_wins'], bar_width, label='White Wins (Model)', color='lightgreen')
bar2 = ax.bar(index, df_results['draws'], bar_width, label='Draws', color='lightgray')
bar3 = ax.bar(index + bar_width, df_results['black_wins'], bar_width, label='Black Wins (Stockfish)', color='salmon')

ax.set_xlabel('Network Combination (Blocks, Channels)')
ax.set_ylabel('Number of Games')
ax.set_title(f'Evaluation Results vs Stockfish ELO {EVAL_STOCKFISH_ELO} for Different Network Combinations (Total {SEARCH_EVAL_GAMES} Games Each)')
ax.set_xticks(index)
ax.set_xticklabels(df_results['Combination'], rotation=45, ha='right')
ax.legend()
ax.grid(axis='y', linestyle='--', alpha=0.7)
plt.tight_layout()
plt.show()

print("\n--- Analysis ---")
print("Look for combinations that have a relatively low training loss AND a good win/draw ratio against Stockfish.")
print("A low training loss indicates the network is learning the self-play data well.")
print("A high win/draw ratio against Stockfish indicates good generalization.")
print("Consider memory usage and training time vs. performance for optimal Colab use.")
print("More complex models (higher blocks/channels) might need more data (increase SEARCH_GAMES_PER_EPOCH/SEARCH_EPOCHS) to shine.")